# Knowledge-distillation-using-BERT
Using Google's pretrained language model, BERT, I aim to train a smaller bidirectional LSTM model with knowledge distillation. The kaggle dataset from Toxic Comment classification is used for this task.
